"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
const crypto_1 = require("crypto");
const fs_extra_1 = require("fs-extra");
const json2csv_1 = require("json2csv");
const ramda_1 = require("ramda");
const readline_1 = require("readline");
const clients_1 = require("../../clients");
const logger_1 = __importDefault(require("../../logger"));
const utils_1 = require("../../utils");
const utils_2 = require("./utils");
const EXPORTS = 'exports';
const [account, workspace] = utils_2.accountAndWorkspace;
const FIELDS = ['from', 'to', 'type', 'endDate'];
const generateListOfRanges = (indexLength) => ramda_1.map((n) => [n * utils_2.MAX_ENTRIES_PER_REQUEST, Math.min((n + 1) * utils_2.MAX_ENTRIES_PER_REQUEST - 1, indexLength)], ramda_1.range(0, Math.ceil(indexLength / utils_2.MAX_ENTRIES_PER_REQUEST)));
const handleExport = async (csvPath) => {
    const rawRoutesIndexFiles = await clients_1.rewriter.routesIndexFiles();
    if (!rawRoutesIndexFiles) {
        logger_1.default.info('No data to be exported.');
        return;
    }
    const routesIndexFiles = ramda_1.prop('routeIndexFiles', rawRoutesIndexFiles);
    const indexHash = await crypto_1.createHash('md5')
        .update(`${account}_${workspace}_${JSON.stringify(rawRoutesIndexFiles)}`)
        .digest('hex');
    const numberOfFiles = ramda_1.sum(ramda_1.compose(ramda_1.map(Number), ramda_1.pluck('fileSize'))(routesIndexFiles));
    if (numberOfFiles === 0) {
        logger_1.default.info('No data to be exported.');
        return;
    }
    const metainfo = await fs_extra_1.readJson(utils_2.METAINFO_FILE).catch(() => ({}));
    const exportMetainfo = metainfo[EXPORTS] || {};
    const listOfRanges = generateListOfRanges(numberOfFiles);
    let counter = exportMetainfo[indexHash] ? exportMetainfo[indexHash].counter : 0;
    let listOfRoutes = exportMetainfo[indexHash] ? exportMetainfo[indexHash].data : [];
    const bar = utils_2.progressBar('Exporting routes...', counter, ramda_1.length(listOfRanges));
    const listener = readline_1.createInterface({ input: process.stdin, output: process.stdout }).on('SIGINT', () => {
        utils_2.saveMetainfo(metainfo, EXPORTS, indexHash, counter, listOfRoutes);
        console.log('\n');
        process.exit();
    });
    for (const [from, to] of listOfRanges.splice(counter)) {
        let result;
        try {
            result = await clients_1.rewriter.exportRedirects(from, to);
        }
        catch (e) {
            utils_2.saveMetainfo(metainfo, EXPORTS, indexHash, counter, listOfRoutes);
            listener.close();
            throw e;
        }
        listOfRoutes = ramda_1.concat(listOfRoutes, result);
        counter++;
        bar.tick();
    }
    const json2csvParser = new json2csv_1.Parser({ fields: FIELDS, delimiter: ';', quote: '' });
    const csv = json2csvParser.parse(listOfRoutes);
    await fs_extra_1.writeFile(`./${csvPath}`, csv);
    logger_1.default.info('Finished!\n');
    listener.close();
    utils_2.deleteMetainfo(metainfo, EXPORTS, indexHash);
};
let retryCount = 0;
exports.default = async (csvPath) => {
    try {
        await handleExport(csvPath);
    }
    catch (e) {
        logger_1.default.error('Error handling export\n');
        utils_2.showGraphQLErrors(e);
        if (utils_1.isVerbose) {
            console.log(e);
        }
        if (retryCount >= utils_2.MAX_RETRIES) {
            process.exit();
        }
        logger_1.default.error(`Retrying in ${utils_2.RETRY_INTERVAL_S} seconds...`);
        logger_1.default.info('Press CTRL+C to abort');
        await utils_2.sleep(utils_2.RETRY_INTERVAL_S * 1000);
        retryCount++;
        await module.exports.default(csvPath);
    }
};
